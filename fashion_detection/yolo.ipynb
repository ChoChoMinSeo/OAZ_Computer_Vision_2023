{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd yolov5\n",
    "%pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data yaml file 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'names': ['Short sleeve top', 'Long sleeve top', 'Short sleeve outwear', 'Long sleeve outwear', 'Vest', 'Sling', 'Shorts', 'Trousers', 'Skirt', 'Short sleeve dress', 'Long sleeve dress', 'Vest dress', 'Sling dress'], 'nc': 13, 'train': './datas/train/image', 'val': './datas/validation/image/'}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open('./datas/data.yaml','w') as f:\n",
    "    yaml.dump({\n",
    "        'names':['Short sleeve top','Long sleeve top','Short sleeve outwear', 'Long sleeve outwear','Vest','Sling','Shorts','Trousers','Skirt','Short sleeve dress','Long sleeve dress','Vest dress','Sling dress'],\n",
    "        'nc':13,\n",
    "        'train':'./datas/train/image',\n",
    "        'val':'./datas/validation/image/'\n",
    "    },f)\n",
    "with open('./datas/data.yaml','r') as f:\n",
    "    print(yaml.load(f,Loader=yaml.FullLoader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset label txt file 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "train_anno_dir = './datas/train/annos/*'\n",
    "train_imgs_dir = './datas/train/image/*'\n",
    "train_save_dir = './datas/train/labels/'\n",
    "valid_anno_dir = './datas/validation/annos/*'\n",
    "valid_imgs_dir = './datas/validation/image/*'\n",
    "valid_save_dir = './datas/validation/labels/'\n",
    "train_annos = glob.glob(train_anno_dir)\n",
    "train_images = glob.glob(train_imgs_dir)\n",
    "valid_annos = glob.glob(valid_anno_dir)\n",
    "valid_images = glob.glob(valid_imgs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "def to_yolo_coord(org_coord,width,height):\n",
    "    x1,y1,x2,y2 = org_coord\n",
    "    w= x2-x1\n",
    "    h = y2-y1\n",
    "    return [str(x1/width),str(y1/height),str(w/width),str(h/height)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datas/train/image\\000001.jpg\n",
      "./datas/train/annos\\000001.json\n"
     ]
    }
   ],
   "source": [
    "print(train_images[0])\n",
    "print(train_annos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191961/191961 [1:52:41<00:00, 28.39it/s]  \n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(train_annos))):\n",
    "    cur_img = Image.open(train_images[i])\n",
    "    w,h = cur_img.size\n",
    "    with open(train_annos[i],'r') as f:\n",
    "        json_ver = json.loads(f.read())\n",
    "        res = []\n",
    "        for item_idx in range(1,5):\n",
    "            try:\n",
    "                temp_coord = json_ver[f'item{item_idx}']['bounding_box']\n",
    "                yolo_coord = to_yolo_coord(temp_coord,w,h)\n",
    "                category = json_ver[f'item{item_idx}']['category_id']\n",
    "                bbox_string = ' '.join(yolo_coord)\n",
    "                res.append(f'{category} {bbox_string}')\n",
    "            except:\n",
    "                break\n",
    "        with open(train_save_dir+str(i+1).zfill(6)+'.txt','w',encoding=\"utf-8\") as txt_f:\n",
    "            txt_f.write('\\n'.join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32153/32153 [17:36<00:00, 30.44it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(valid_annos))):\n",
    "    cur_img = Image.open(valid_images[i])\n",
    "    w,h = cur_img.size\n",
    "    with open(valid_annos[i],'r') as f:\n",
    "        json_ver = json.loads(f.read())\n",
    "        res = []\n",
    "        for item_idx in range(1,5):\n",
    "            try:\n",
    "                temp_coord = json_ver[f'item{item_idx}']['bounding_box']\n",
    "                yolo_coord = to_yolo_coord(temp_coord,w,h)\n",
    "                category = json_ver[f'item{item_idx}']['category_id']\n",
    "                bbox_string = ' '.join(yolo_coord)\n",
    "                res.append(f'{category} {bbox_string}')\n",
    "            except:\n",
    "                break\n",
    "        with open(valid_save_dir+str(i+1).zfill(6)+'.txt','w',encoding=\"utf-8\") as txt_f:\n",
    "            txt_f.write('\\n'.join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_save_dir = glob.glob('./datas/train/labels/*')\n",
    "train_imgs_dir = './datas/train/image/'\n",
    "# len(train_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191961/191961 [14:38<00:00, 218.54it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(train_save_dir))):\n",
    "    with open(train_save_dir[i],'r') as f:\n",
    "        with open(train_imgs_dir+str(i+1).zfill(6)+'.txt','w',encoding=\"utf-8\") as txt_f:\n",
    "            txt_f.write(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 반만 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from os import remove\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_imgs_dir = './datas/train/image/*.jpg'\n",
    "train_txt_dir = './datas/train/image/*.txt'\n",
    "train_remain_dir = './datas/train/half/'\n",
    "valid_imgs_dir = './datas/validation/image/*.jpg'\n",
    "valid_txt_dir = './datas/validation/image/*.txt'\n",
    "valid_remain_dir = './datas/validation/half/'\n",
    "train_imgs = glob.glob(train_imgs_dir)\n",
    "train_txts = glob.glob(train_txt_dir)\n",
    "valid_imgs = glob.glob(valid_imgs_dir)\n",
    "valid_txts = glob.glob(valid_txt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47990/47990 [26:19<00:00, 30.39it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(train_imgs)-1,len(train_imgs)//2,-1)):\n",
    "    cur_img = Image.open(train_imgs[i])\n",
    "    cur_img.save(train_remain_dir+str(i).zfill(6)+'.jpg')\n",
    "    remove(train_imgs[i])\n",
    "    with open(train_txts[i],'r') as f:\n",
    "        with open(train_remain_dir+str(i).zfill(6)+'.txt','w') as txt_f:\n",
    "            txt_f.write(f.read())\n",
    "    remove(train_txts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16076/16076 [08:10<00:00, 32.74it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(valid_imgs)-1,len(valid_imgs)//2,-1)):\n",
    "    cur_img = Image.open(valid_imgs[i])\n",
    "    cur_img.save(valid_remain_dir+str(i).zfill(6)+'.jpg')\n",
    "    remove(valid_imgs[i])\n",
    "    with open(valid_txts[i],'r') as f:\n",
    "        with open(valid_remain_dir+str(i).zfill(6)+'.txt','w') as txt_f:\n",
    "            txt_f.write(f.read())\n",
    "    remove(valid_txts[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
