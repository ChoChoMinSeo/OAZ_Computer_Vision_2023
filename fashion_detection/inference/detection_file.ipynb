{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.common import DetectMultiBackend\n",
    "from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
    "from utils.general import (LOGGER, Profile, check_imshow, cv2,non_max_suppression, scale_boxes, xyxy2xywh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model summary: 212 layers, 20905467 parameters, 0 gradients, 48.0 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = './weights/best.pt'\n",
    "\n",
    "model = DetectMultiBackend(weights, device=device)\n",
    "stride, names, pt = model.stride, model.names, model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING  Environment does not support cv2.imshow() or PIL Image.show()\n",
      "\n",
      "1/1: 0...  Success (inf frames 1280x720 at 30.00 FPS)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataloader\n",
    "bs = 1  # batch_size\n",
    "webcam = True\n",
    "source='../datas/test/4.jpg'  # file/dir/URL/glob/screen/0(webcam)\n",
    "imgsz = (224,224)\n",
    "if webcam:\n",
    "    source = '0'\n",
    "    view_img = check_imshow(warn=True)\n",
    "    dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=1)\n",
    "    bs = len(dataset)\n",
    "else:\n",
    "    source='../datas/test/4.jpg' \n",
    "    dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen, windows, dt = 0, [], (Profile(), Profile(), Profile())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_img = True\n",
    "for path, im, im0s, vid_cap, s in dataset:\n",
    "    with dt[0]:\n",
    "        im = torch.from_numpy(im).to(model.device)\n",
    "        im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
    "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "        if len(im.shape) == 3:\n",
    "            im = im[None]  # expand for batch dim\n",
    "    with dt[1]:\n",
    "        pred = model(im)\n",
    "    with dt[2]:\n",
    "        pred = non_max_suppression(pred, 0.15, 0.45, None, True, max_det=2)\n",
    "    for i, det in enumerate(pred):  # per image\n",
    "        seen += 1\n",
    "        if webcam:  # batch_size >= 1\n",
    "            p, im0, frame = path[i], im0s[i].copy(), dataset.count\n",
    "            s += f'{i}: '\n",
    "        else:\n",
    "            p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)\n",
    "        h,w,_ = im0.shape\n",
    "        p = Path(p)\n",
    "        save_path = f'./res/{p.name}' # im.jpg\n",
    "        s += '%gx%g ' % im.shape[2:]  # print string\n",
    "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "        # annotator = Annotator(im0, line_width=3, example=str(names))\n",
    "        if len(det):\n",
    "            # Rescale boxes from img_size to im0 size\n",
    "            det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "            # Write results\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                c = int(cls)  # integer class\n",
    "                label = names[c]\n",
    "                x1,y1,x2,y2 = torch.tensor(xyxy)\n",
    "                confidence = float(conf)\n",
    "                \n",
    "                xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()\n",
    "                x1 = int(xywh[0]*w)\n",
    "                y1 = int(xywh[1]*h)\n",
    "                x2 = int((xywh[0]+xywh[2])*w)\n",
    "                y2 = int((xywh[1]+xywh[3])*h)\n",
    "                \n",
    "                c = int(cls)  # integer class\n",
    "                label = f'{names[c]} {conf:.2f}'\n",
    "                cv2.rectangle(im0,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "                cv2.putText(im0,label,fontFace=cv2.FONT_HERSHEY_SIMPLEX,org = (x1,y1),fontScale = 0.5,color = (255,0,0),thickness=2)\n",
    "                    \n",
    "        # Stream results\n",
    "    if view_img:\n",
    "        cv2.imshow(str(p), im0)\n",
    "        cv2.waitKey(1)  # 1 millisecond\n",
    "    if save_img:\n",
    "        if dataset.mode == 'image':\n",
    "            cv2.imwrite(save_path, im0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fashion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
